import ollama
from config import Config
from utils.web_search import WebSearcher
from utils.monitoring import SearchMonitor
from rich.console import Console
from rich.panel import Panel
from rich.markdown import Markdown
import threading
import time
import json
from datetime import datetime

class DeepSeekAgent:
    def __init__(self, config: Config):
        self.config = config
        self.console = Console()
        self.monitor = SearchMonitor(config.LOG_FILE)
        self.monitor.console_monitor = config.ENABLE_CONSOLE_MONITOR
        self.searcher = WebSearcher(self.monitor, config.MAX_SEARCH_RESULTS)
        
        # Verificar se o modelo estÃ¡ disponÃ­vel
        self._check_model()
    
    def _check_model(self):
        """Verifica se o modelo DeepSeek estÃ¡ disponÃ­vel no Ollama"""
        try:
            # MÃ©todo corrigido para verificar modelos
            models_response = ollama.list()
            
            self.console.print(f"[dim]Tipo da resposta: {type(models_response)}[/dim]")
            self.console.print(f"[dim]Atributos disponÃ­veis: {dir(models_response)}[/dim]")
            
            # Acessar corretamente a lista de modelos
            model_names = []
            model_details = []
            
            if hasattr(models_response, 'models') and models_response.models:
                for model in models_response.models:
                    model_name = model.model  # Acessar via atributo 'model'
                    model_names.append(model_name)
                    model_details.append({
                        'name': model_name,
                        'size': model.size,
                        'modified': model.modified_at,
                        'parameters': getattr(model.details, 'parameter_size', 'N/A') if model.details else 'N/A'
                    })
            
            self.console.print(f"[dim]Modelos encontrados: {model_names}[/dim]")
            
            if not model_names:
                self.console.print("âŒ [red]Nenhum modelo encontrado no Ollama[/red]")
                raise Exception("Nenhum modelo disponÃ­vel")
            
            # Encontrar modelos DeepSeek
            deepseek_models = [model for model in model_details if 'deepseek' in model['name'].lower()]
            
            if deepseek_models:
                # Usar o primeiro modelo DeepSeek encontrado
                selected_model = deepseek_models[0]
                self.config.MODEL_NAME = selected_model['name']
                
                self.console.print(Panel(
                    f"âœ… [green]Modelo selecionado:[/green] {self.config.MODEL_NAME}\n"
                    f"ğŸ“Š [cyan]Tamanho:[/cyan] {selected_model['size']/1024/1024/1024:.1f}GB\n"
                    f"âš™ï¸ [yellow]ParÃ¢metros:[/yellow] {selected_model['parameters']}\n"
                    f"ğŸ“… [magenta]Modificado:[/magenta] {selected_model['modified'].strftime('%Y-%m-%d %H:%M')}",
                    title="ğŸ¤– Modelo Carregado",
                    border_style="green"
                ))
            else:
                # Usar o primeiro modelo disponÃ­vel
                selected_model = model_details[0]
                self.config.MODEL_NAME = selected_model['name']
                self.console.print(Panel(
                    f"âš ï¸ [yellow]Usando modelo disponÃ­vel:[/yellow] {self.config.MODEL_NAME}\n"
                    f"ğŸ“Š [cyan]Tamanho:[/cyan] {selected_model['size']/1024/1024/1024:.1f}GB",
                    title="ğŸ¤– Modelo Alternativo",
                    border_style="yellow"
                ))
                
        except Exception as e:
            self.console.print(f"âŒ Erro ao conectar com Ollama: {e}", style="bold red")
            self.console.print("\nğŸ”§ [yellow]SoluÃ§Ãµes possÃ­veis:[/yellow]")
            self.console.print("1. Verifique se o Ollama estÃ¡ rodando: ollama serve")
            self.console.print("2. Instale um modelo: ollama pull deepseek-coder")
            raise
    
    def _should_search(self, user_input: str) -> bool:
        """
        Determina se uma pesquisa na internet Ã© necessÃ¡ria
        """
        search_keywords = [
            'pesquisar', 'buscar', 'encontrar', 'procurar',
            'notÃ­cias', 'atual', 'recente', 'hoje', 'agora',
            'internet', 'web', 'online', 'duckduckgo', 'google',
            'qual Ã©', 'quem Ã©', 'o que Ã©', 'quando', 'onde',
            'como', 'por que', 'atualizaÃ§Ã£o', 'Ãºltimas',
            'novidades', 'preÃ§o', 'cotaÃ§Ã£o', 'clima', 'tempo'
        ]
        
        user_input_lower = user_input.lower()
        
        # Verificar palavras-chave
        for keyword in search_keywords:
            if keyword in user_input_lower:
                return True
        
        # Verificar se Ã© uma pergunta factual
        question_prefixes = [
            'qual ', 'quem ', 'o que ', 'onde ', 'quando ',
            'como ', 'por que ', 'quantos ', 'quantas ', 'quanto '
        ]
        if any(user_input_lower.startswith(prefix) for prefix in question_prefixes):
            return True
        
        # Verificar se pergunta sobre eventos atuais
        current_events = ['eleiÃ§Ã£o', 'presidente', 'governo', 'mercado', 'bolsa', 'bitcoin']
        if any(event in user_input_lower for event in current_events):
            return True
        
        return False
    
    def _extract_search_query(self, user_input: str) -> str:
        """
        Extrai a query de pesquisa do input do usuÃ¡rio
        """
        # Remove comandos explÃ­citos de pesquisa
        remove_phrases = [
            'pesquise por', 'busque por', 'encontre', 'procure por',
            'pesquisar', 'buscar', 'encontrar', 'procurar',
            'quero saber sobre', 'preciso de informaÃ§Ãµes sobre',
            'me mostre sobre', 'me fale sobre'
        ]
        
        query = user_input.lower()
        for phrase in remove_phrases:
            query = query.replace(phrase, '')
        
        return query.strip()
    
    def generate_response(self, user_input: str, search_results: str = "") -> str:
        """
        Gera uma resposta usando o modelo DeepSeek
        """
        try:
            # Construir o prompt
            if search_results and search_results != "Nenhum resultado encontrado na pesquisa.":
                prompt = f"""Com base nos resultados de pesquisa abaixo e no seu conhecimento, responda Ã  pergunta do usuÃ¡rio de forma Ãºtil, precisa e bem estruturada.

RESULTADOS DA PESQUISA:
{search_results}

PERGUNTA DO USUÃRIO: {user_input}

INSTRUÃ‡Ã•ES:
- Responda em portuguÃªs claro e natural
- Seja informativo e direto
- Use os resultados da pesquisa quando relevantes
- Se os resultados nÃ£o forem Ãºteis, use seu conhecimento
- Formate a resposta de forma organizada"""
            else:
                prompt = f"""Responda Ã  seguinte pergunta do usuÃ¡rio de forma Ãºtil, precisa e bem estruturada:

PERGUNTA: {user_input}

INSTRUÃ‡Ã•ES:
- Responda em portuguÃªs claro e natural
- Seja informativo e direto
- Formate a resposta de forma organizada"""
            
            # Gerar resposta
            self.console.print(f"[dim]ğŸ”„ Gerando resposta com {self.config.MODEL_NAME}...[/dim]")
            
            response = ollama.generate(
                model=self.config.MODEL_NAME,
                prompt=prompt,
                options={
                    'temperature': 0.7,
                    'top_p': 0.9,
                    'num_predict': 1000
                }
            )
            
            # Acessar a resposta corretamente
            response_text = response.response
            
            # Registrar no monitor
            self.monitor.log_response(prompt, response_text)
            
            return response_text
            
        except Exception as e:
            error_msg = f"âŒ Erro ao gerar resposta: {e}"
            self.monitor.logger.error(error_msg)
            self.console.print(f"[dim]Detalhes do erro: {type(e).__name__}[/dim]")
            return error_msg
    
    def process_query(self, user_input: str) -> str:
        """
        Processa a query do usuÃ¡rio e retorna uma resposta
        """
        self.console.print(Panel(
            f"ğŸ’­ [bold blue]UsuÃ¡rio:[/bold blue] {user_input}",
            border_style="blue"
        ))
        
        # Determinar se precisa pesquisar
        if self._should_search(user_input):
            search_query = self._extract_search_query(user_input)
            self.console.print(f"ğŸ” [yellow]Realizando pesquisa: '{search_query}'[/yellow]")
            search_context = self.searcher.get_search_context(search_query)
            
            # Gerar resposta com contexto da pesquisa
            response = self.generate_response(user_input, search_context)
        else:
            # Gerar resposta sem pesquisa
            response = self.generate_response(user_input)
        
        return response
    
    def chat_loop(self):
        """
        Loop principal de chat
        """
        self.console.print(Panel(
            f"ğŸ¤– [bold green]Agente DeepSeek Ativado[/bold green]\n"
            f"ğŸ“š Modelo: {self.config.MODEL_NAME}\n"
            f"ğŸ” Pesquisas automÃ¡ticas ativadas\n"
            f"ğŸŒ Conectado ao DuckDuckGo\n"
            f"ğŸ“Š Monitoramento ativo\n"
            f"\nğŸ’¬ [bold]Comandos:[/bold]\n"
            f"  â€¢ 'sair' - Encerrar\n"
            f"  â€¢ 'historico' - Pesquisas recentes\n"
            f"  â€¢ 'modelos' - Listar modelos\n"
            f"  â€¢ 'teste' - Testar modelo\n"
            f"  â€¢ 'status' - Status do sistema",
            border_style="green"
        ))
        
        while True:
            try:
                user_input = input("\nğŸ‘¤ VocÃª: ").strip()
                
                if user_input.lower() == 'sair':
                    self.console.print("ğŸ‘‹ AtÃ© logo!", style="bold yellow")
                    break
                elif user_input.lower() == 'historico':
                    self._show_search_history()
                    continue
                elif user_input.lower() == 'modelos':
                    self._show_available_models()
                    continue
                elif user_input.lower() == 'teste':
                    self._test_model()
                    continue
                elif user_input.lower() == 'status':
                    self._show_system_status()
                    continue
                elif not user_input:
                    continue
                
                # Processar a query
                start_time = time.time()
                response = self.process_query(user_input)
                response_time = time.time() - start_time
                
                # Exibir resposta
                self.console.print(Panel(
                    Markdown(response),
                    title=f"ğŸ¤– DeepSeek ({response_time:.1f}s)",
                    border_style="green"
                ))
                
            except KeyboardInterrupt:
                self.console.print("\nğŸ‘‹ Encerrado pelo usuÃ¡rio", style="bold yellow")
                break
            except Exception as e:
                self.console.print(f"âŒ Erro: {e}", style="bold red")
    
    def _show_search_history(self):
        """Mostra o histÃ³rico de pesquisas"""
        history = self.searcher.get_search_history()
        
        if not history:
            self.console.print("ğŸ“ Nenhuma pesquisa realizada ainda.", style="yellow")
            return
        
        self.console.print("\nğŸ“Š [bold]HistÃ³rico de Pesquisas:[/bold]")
        for i, search in enumerate(history[-5:], 1):
            timestamp = time.strftime('%H:%M:%S', time.localtime(search['timestamp']))
            self.console.print(
                f"  {i}. [{timestamp}] '{search['query']}' - "
                f"{search['results_count']} resultados"
            )
    
    def _show_available_models(self):
        """Mostra modelos disponÃ­veis"""
        try:
            models_response = ollama.list()
            self.console.print("\nğŸ“š [bold]Modelos DisponÃ­veis:[/bold]")
            
            if hasattr(models_response, 'models') and models_response.models:
                for model in models_response.models:
                    size_gb = model.size / 1024 / 1024 / 1024
                    params = getattr(model.details, 'parameter_size', 'N/A') if model.details else 'N/A'
                    
                    self.console.print(f"  âœ… {model.model}")
                    self.console.print(f"     ğŸ“Š {size_gb:.1f}GB | âš™ï¸ {params} | ğŸ“… {model.modified_at.strftime('%d/%m %H:%M')}")
            else:
                self.console.print("  â„¹ï¸  Nenhum modelo encontrado")
                
        except Exception as e:
            self.console.print(f"  âŒ Erro ao listar modelos: {e}")
    
    def _test_model(self):
        """Testa o modelo com uma pergunta simples"""
        self.console.print("\nğŸ§ª [bold]Testando o modelo...[/bold]")
        
        test_prompts = [
            "Explique o que Ã© Python em uma frase.",
            "Qual Ã© a capital do Brasil?",
            "Como fazer um bolo simples?"
        ]
        
        for i, prompt in enumerate(test_prompts, 1):
            self.console.print(f"\nğŸ“ Teste {i}: {prompt}")
            
            try:
                start_time = time.time()
                response = ollama.generate(
                    model=self.config.MODEL_NAME,
                    prompt=prompt
                )
                response_time = time.time() - start_time
                
                if hasattr(response, 'response'):
                    self.console.print(Panel(
                        response.response,
                        title=f"âœ… Resposta ({response_time:.1f}s)",
                        border_style="green"
                    ))
                else:
                    self.console.print("âŒ [red]Resposta inesperada do modelo[/red]")
                    
            except Exception as e:
                self.console.print(f"âŒ [red]Erro no teste: {e}[/red]")
    
    def _show_system_status(self):
        """Mostra status do sistema"""
        try:
            models_response = ollama.list()
            model_count = len(models_response.models) if hasattr(models_response, 'models') else 0
            
            search_history = self.searcher.get_search_history()
            search_count = len(search_history)
            
            self.console.print(Panel(
                f"ğŸ¤– [bold]Status do Sistema[/bold]\n\n"
                f"ğŸ“š Modelos carregados: {model_count}\n"
                f"ğŸ” Pesquisas realizadas: {search_count}\n"
                f"âš™ï¸  Modelo atual: {self.config.MODEL_NAME}\n"
                f"ğŸ•’ Hora do sistema: {datetime.now().strftime('%H:%M:%S')}",
                border_style="blue"
            ))
            
        except Exception as e:
            self.console.print(f"âŒ [red]Erro ao verificar status: {e}[/red]")

def main():
    # ConfiguraÃ§Ã£o
    config = Config()
    
    try:
        # Inicializar agente
        agent = DeepSeekAgent(config)
        
        # Iniciar chat
        agent.chat_loop()
        
    except Exception as e:
        console = Console()
        console.print(f"âŒ Erro ao iniciar agente: {e}", style="bold red")
        console.print("\nğŸ’¡ Execute 'ollama serve' em outro terminal e tente novamente.")

if __name__ == "__main__":
    main()