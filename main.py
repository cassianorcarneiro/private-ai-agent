# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#
# AEGIS-MIND
# CASSIANO RIBEIRO CARNEIRO
# V1
# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#

# Import frameworks

import ollama
from config import Config
from utils.web_search import WebSearcher
from utils.monitoring import SearchMonitor
from rich.console import Console
from rich.panel import Panel
from rich.markdown import Markdown
import time
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed

# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#
# 
# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#

class DeepSeekAgent:

    # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#
    # 
    # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#

    def __init__(self, config: Config):
        self.config = config
        self.console = Console()
        self.monitor = SearchMonitor(config.LOG_FILE)
        self.monitor.console_monitor = config.ENABLE_CONSOLE_MONITOR
        self.searcher = WebSearcher(self.monitor, config.MAX_SEARCH_RESULTS)
        
        # Verificar se o modelo est√° dispon√≠vel
        self._check_model()
    
    # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#
    # 
    # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#

    def _check_model(self):
        """Verifica se o modelo DeepSeek est√° dispon√≠vel no Ollama"""
        try:
            # M√©todo corrigido para verificar modelos
            models_response = ollama.list()
            
            self.console.print(f"[dim]Tipo da resposta: {type(models_response)}[/dim]")
            self.console.print(f"[dim]Atributos dispon√≠veis: {dir(models_response)}[/dim]")
            
            # Acessar corretamente a lista de modelos
            model_names = []
            model_details = []
            
            if hasattr(models_response, 'models') and models_response.models:
                for model in models_response.models:
                    model_name = model.model  # Acessar via atributo 'model'
                    model_names.append(model_name)
                    model_details.append({
                        'name': model_name,
                        'size': model.size,
                        'modified': model.modified_at,
                        'parameters': getattr(model.details, 'parameter_size', 'N/A') if model.details else 'N/A'
                    })
            
            self.console.print(f"[dim]Modelos encontrados: {model_names}[/dim]")
            
            if not model_names:
                self.console.print("‚ùå [red]Nenhum modelo encontrado no Ollama[/red]")
                raise Exception("Nenhum modelo dispon√≠vel")
            
            # Encontrar modelos DeepSeek
            deepseek_models = [model for model in model_details if 'deepseek' in model['name'].lower()]
            
            if deepseek_models:
                # Usar o primeiro modelo DeepSeek encontrado
                selected_model = deepseek_models[0]
                self.config.MODEL_NAME = selected_model['name']
                
                self.console.print(Panel(
                    f"‚úÖ [green]Modelo selecionado:[/green] {self.config.MODEL_NAME}\n"
                    f"üìä [cyan]Tamanho:[/cyan] {selected_model['size']/1024/1024/1024:.1f}GB\n"
                    f"‚öôÔ∏è [yellow]Par√¢metros:[/yellow] {selected_model['parameters']}\n"
                    f"üìÖ [magenta]Modificado:[/magenta] {selected_model['modified'].strftime('%Y-%m-%d %H:%M')}",
                    title="ü§ñ Modelo Carregado",
                    border_style="green"
                ))
            else:
                # Usar o primeiro modelo dispon√≠vel
                selected_model = model_details[0]
                self.config.MODEL_NAME = selected_model['name']
                self.console.print(Panel(
                    f"‚ö†Ô∏è [yellow]Usando modelo dispon√≠vel:[/yellow] {self.config.MODEL_NAME}\n"
                    f"üìä [cyan]Tamanho:[/cyan] {selected_model['size']/1024/1024/1024:.1f}GB",
                    title="ü§ñ Modelo Alternativo",
                    border_style="yellow"
                ))
                
        except Exception as e:
            self.console.print(f"‚ùå Erro ao conectar com Ollama: {e}", style="bold red")
            self.console.print("\nüîß [yellow]Solu√ß√µes poss√≠veis:[/yellow]")
            self.console.print("1. Verifique se o Ollama est√° rodando: ollama serve")
            self.console.print("2. Instale um modelo: ollama pull deepseek-coder")
            raise
    
    # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#
    # 
    # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#

    def _extract_search_query(self, user_input: str) -> str:
        """
        Extrai a query de pesquisa do input do usu√°rio
        """
        # Remove comandos expl√≠citos de pesquisa
        remove_phrases = [
            'pesquise por', 'busque por', 'encontre', 'procure por',
            'pesquisar', 'buscar', 'encontrar', 'procurar',
            'quero saber sobre', 'preciso de informa√ß√µes sobre',
            'me mostre sobre', 'me fale sobre'
        ]
        
        query = user_input.lower()
        for phrase in remove_phrases:
            query = query.replace(phrase, '')
        
        return query.strip()
    
    # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#
    # 
    # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#

    def generate_response(self, user_input: str, search_results: str = "") -> str:
        """
        Gera uma resposta usando o modelo DeepSeek
        """
        try:
            if self.config.ENABLE_MULTI_AGENT:
                return self._generate_collaborative_response(user_input, search_results)

            # Construir o prompt
            if search_results and search_results != "Nenhum resultado encontrado na pesquisa.":
                prompt = f"""Com base nos resultados de pesquisa abaixo e no seu conhecimento, responda √† pergunta do usu√°rio de forma √∫til, precisa e bem estruturada.

RESULTADOS DA PESQUISA:
{search_results}

PERGUNTA DO USU√ÅRIO: {user_input}

INSTRU√á√ïES:
- Responda em portugu√™s claro e natural
- Seja informativo e direto
- Use os resultados da pesquisa quando relevantes
- Se os resultados n√£o forem √∫teis, use seu conhecimento
- Formate a resposta de forma organizada"""
            else:
                prompt = f"""Responda √† seguinte pergunta do usu√°rio de forma √∫til, precisa e bem estruturada:

PERGUNTA: {user_input}

INSTRU√á√ïES:
- Responda em portugu√™s claro e natural
- Seja informativo e direto
- Formate a resposta de forma organizada"""
            
            # Gerar resposta
            self.console.print(f"[dim]üîÑ Gerando resposta com {self.config.MODEL_NAME}...[/dim]")
            
            response = ollama.generate(
                model=self.config.MODEL_NAME,
                prompt=prompt,
                options={
                    'temperature': 0.7,
                    'top_p': 0.9,
                    'num_predict': 1000
                }
            )
            
            # Acessar a resposta corretamente
            response_text = response.response
            
            # Registrar no monitor
            self.monitor.log_response(prompt, response_text)
            
            return response_text
            
        except Exception as e:
            error_msg = f"‚ùå Erro ao gerar resposta: {e}"
            self.monitor.logger.error(error_msg)
            self.console.print(f"[dim]Detalhes do erro: {type(e).__name__}[/dim]")
            return error_msg

    # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#
    # 
    # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#

    def _build_agent_prompt(self, role: dict, user_input: str, search_results: str) -> str:
        """Cria o prompt espec√≠fico para um agente colaborativo."""
        search_context = ""
        if search_results and search_results != "Nenhum resultado encontrado na pesquisa.":
            search_context = f"\n\nCONTEXTO DE PESQUISA:\n{search_results}"

        return (
            "Voc√™ √© um agente especializado em colabora√ß√£o.\n"
            f"Seu papel: {role['name']}.\n"
            f"Objetivo: {role['goal']}\n"
            "Responda em portugu√™s claro e natural.\n"
            "Seja conciso, direto e traga apenas informa√ß√µes √∫teis.\n"
            "Use listas quando apropriado.\n"
            f"{search_context}\n\n"
            f"PERGUNTA DO USU√ÅRIO: {user_input}"
        )

    # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#
    # 
    # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#

    def _run_agent(self, role: dict, user_input: str, search_results: str) -> dict:
        """Executa um agente colaborativo e retorna sua resposta."""
        prompt = self._build_agent_prompt(role, user_input, search_results)
        try:
            response = ollama.generate(
                model=self.config.MODEL_NAME,
                prompt=prompt,
                options={
                    "temperature": role.get("temperature", 0.4),
                    "top_p": 0.9,
                    "num_predict": 800,
                },
            )
            response_text = response.response
            self.monitor.log_response(prompt, response_text)
            return {"name": role["name"], "response": response_text}
        except Exception as e:
            error_msg = f"Erro no agente {role['name']}: {e}"
            self.monitor.logger.error(error_msg)
            return {"name": role["name"], "response": error_msg}

    # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#
    # 
    # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#

    def _generate_collaborative_response(self, user_input: str, search_results: str) -> str:
        """Gera resposta final combinando m√∫ltiplos agentes."""
        self.console.print("[dim]ü§ù Executando agentes colaborativos...[/dim]")
        agent_outputs = []

        with ThreadPoolExecutor(max_workers=len(self.config.AGENT_ROLES)) as executor:
            futures = [
                executor.submit(self._run_agent, role, user_input, search_results)
                for role in self.config.AGENT_ROLES
            ]
            for future in as_completed(futures):
                agent_outputs.append(future.result())

        agent_outputs.sort(key=lambda item: item["name"])

        synthesis_prompt = (
            "Voc√™ √© o coordenador final que deve sintetizar as respostas abaixo.\n"
            "Combine as contribui√ß√µes dos agentes em uma resposta √∫nica, clara e √∫til.\n"
            "Seja direto, bem estruturado, e mencione limita√ß√µes quando necess√°rio.\n"
            "Responda em portugu√™s claro e natural.\n\n"
        )

        for output in agent_outputs:
            synthesis_prompt += (
                f"AGENTE: {output['name']}\n"
                f"RESPOSTA:\n{output['response']}\n\n"
            )

        synthesis_prompt += f"PERGUNTA ORIGINAL: {user_input}"

        try:
            response = ollama.generate(
                model=self.config.MODEL_NAME,
                prompt=synthesis_prompt,
                options={
                    "temperature": 0.6,
                    "top_p": 0.9,
                    "num_predict": 1200,
                },
            )
            response_text = response.response
            self.monitor.log_response(synthesis_prompt, response_text)
            return response_text
        except Exception as e:
            error_msg = f"‚ùå Erro ao sintetizar respostas: {e}"
            self.monitor.logger.error(error_msg)
            return error_msg
    
    # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#
    # 
    # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#

    def process_query(self, user_input: str) -> str:
        """
        Processa a query do usu√°rio e retorna uma resposta
        """
        self.console.print(Panel(
            f"üí≠ [bold blue]Usu√°rio:[/bold blue] {user_input}",
            border_style="blue"
        ))
        
        # Realiza a pesquisa

        search_query = self._extract_search_query(user_input)
        self.console.print(f"üîç [yellow]Realizando pesquisa: '{search_query}'[/yellow]")
        search_context = self.searcher.get_search_context(search_query)
        
        # Gerar resposta com contexto da pesquisa
        response = self.generate_response(user_input, search_context)
        
        return response
    
    # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#
    # 
    # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#

    def chat_loop(self):
        """
        Loop principal de chat
        """
        self.console.print(Panel(
            f"ü§ñ [bold green]Agente DeepSeek Ativado[/bold green]\n"
            f"üìö Modelo: {self.config.MODEL_NAME}\n"
            f"üîç Pesquisas autom√°ticas ativadas\n"
            f"üåê Conectado ao DuckDuckGo\n"
            f"üìä Monitoramento ativo\n"
            f"ü§ù Colabora√ß√£o multi-agente: {'ativada' if self.config.ENABLE_MULTI_AGENT else 'desativada'}\n"
            f"\nüí¨ [bold]Comandos:[/bold]\n"
            f"  ‚Ä¢ 'sair' - Encerrar\n"
            f"  ‚Ä¢ 'historico' - Pesquisas recentes\n"
            f"  ‚Ä¢ 'modelos' - Listar modelos\n"
            f"  ‚Ä¢ 'teste' - Testar modelo\n"
            f"  ‚Ä¢ 'status' - Status do sistema",
            border_style="green"
        ))
        
        while True:
            try:
                user_input = input("\nüë§ Voc√™: ").strip()
                
                if user_input.lower() == 'sair':
                    self.console.print("üëã At√© logo!", style="bold yellow")
                    break
                elif user_input.lower() == 'historico':
                    self._show_search_history()
                    continue
                elif user_input.lower() == 'modelos':
                    self._show_available_models()
                    continue
                elif user_input.lower() == 'teste':
                    self._test_model()
                    continue
                elif user_input.lower() == 'status':
                    self._show_system_status()
                    continue
                elif user_input.lower() == 'agentes':
                    self._show_agent_roles()
                    continue
                elif not user_input:
                    continue
                
                # Processar a query
                start_time = time.time()
                response = self.process_query(user_input)
                response_time = time.time() - start_time
                
                # Exibir resposta
                self.console.print(Panel(
                    Markdown(response),
                    title=f"ü§ñ DeepSeek ({response_time:.1f}s)",
                    border_style="green"
                ))
                
            except KeyboardInterrupt:
                self.console.print("\nüëã Encerrado pelo usu√°rio", style="bold yellow")
                break
            except Exception as e:
                self.console.print(f"‚ùå Erro: {e}", style="bold red")
    
    # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#
    # 
    # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#

    def _show_search_history(self):
        """Mostra o hist√≥rico de pesquisas"""
        history = self.searcher.get_search_history()
        
        if not history:
            self.console.print("üìù Nenhuma pesquisa realizada ainda.", style="yellow")
            return
        
        self.console.print("\nüìä [bold]Hist√≥rico de Pesquisas:[/bold]")
        for i, search in enumerate(history[-5:], 1):
            timestamp = time.strftime('%H:%M:%S', time.localtime(search['timestamp']))
            self.console.print(
                f"  {i}. [{timestamp}] '{search['query']}' - "
                f"{search['results_count']} resultados"
            )
    
    # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#
    # 
    # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#

    def _show_available_models(self):
        """Mostra modelos dispon√≠veis"""
        try:
            models_response = ollama.list()
            self.console.print("\nüìö [bold]Modelos Dispon√≠veis:[/bold]")
            
            if hasattr(models_response, 'models') and models_response.models:
                for model in models_response.models:
                    size_gb = model.size / 1024 / 1024 / 1024
                    params = getattr(model.details, 'parameter_size', 'N/A') if model.details else 'N/A'
                    
                    self.console.print(f"  ‚úÖ {model.model}")
                    self.console.print(f"     üìä {size_gb:.1f}GB | ‚öôÔ∏è {params} | üìÖ {model.modified_at.strftime('%d/%m %H:%M')}")
            else:
                self.console.print("  ‚ÑπÔ∏è  Nenhum modelo encontrado")
                
        except Exception as e:
            self.console.print(f"  ‚ùå Erro ao listar modelos: {e}")
    
    # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#
    # 
    # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#

    def _test_model(self):
        """Testa o modelo com uma pergunta simples"""
        self.console.print("\nüß™ [bold]Testando o modelo...[/bold]")
        
        test_prompts = [
            "Explique o que √© Python em uma frase.",
            "Qual √© a capital do Brasil?",
            "Como fazer um bolo simples?"
        ]
        
        for i, prompt in enumerate(test_prompts, 1):
            self.console.print(f"\nüìù Teste {i}: {prompt}")
            
            try:
                start_time = time.time()
                response = ollama.generate(
                    model=self.config.MODEL_NAME,
                    prompt=prompt
                )
                response_time = time.time() - start_time
                
                if hasattr(response, 'response'):
                    self.console.print(Panel(
                        response.response,
                        title=f"‚úÖ Resposta ({response_time:.1f}s)",
                        border_style="green"
                    ))
                else:
                    self.console.print("‚ùå [red]Resposta inesperada do modelo[/red]")
                    
            except Exception as e:
                self.console.print(f"‚ùå [red]Erro no teste: {e}[/red]")
    
    # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#
    # 
    # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#

    def _show_system_status(self):
        """Mostra status do sistema"""
        try:
            models_response = ollama.list()
            model_count = len(models_response.models) if hasattr(models_response, 'models') else 0
            
            search_history = self.searcher.get_search_history()
            search_count = len(search_history)
            
            self.console.print(Panel(
                f"ü§ñ [bold]Status do Sistema[/bold]\n\n"
                f"üìö Modelos carregados: {model_count}\n"
                f"üîç Pesquisas realizadas: {search_count}\n"
                f"‚öôÔ∏è  Modelo atual: {self.config.MODEL_NAME}\n"
                f"ü§ù Multi-agente: {'ativo' if self.config.ENABLE_MULTI_AGENT else 'inativo'}\n"
                f"üïí Hora do sistema: {datetime.now().strftime('%H:%M:%S')}",
                border_style="blue"
            ))
            
        except Exception as e:
            self.console.print(f"‚ùå [red]Erro ao verificar status: {e}[/red]")

    # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#
    # 
    # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#

    def _show_agent_roles(self):
        """Exibe os agentes colaborativos configurados."""
        if not self.config.AGENT_ROLES:
            self.console.print("ü§ù Nenhum agente colaborativo configurado.", style="yellow")
            return

        self.console.print("\nü§ù [bold]Agentes Colaborativos:[/bold]")
        for role in self.config.AGENT_ROLES:
            self.console.print(f"  ‚úÖ {role['name']}: {role['goal']}")

# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#
# 
# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#

def main():
    # Configura√ß√£o
    config = Config()
    
    try:
        # Inicializar agente
        agent = DeepSeekAgent(config)
        
        # Iniciar chat
        agent.chat_loop()
        
    except Exception as e:
        console = Console()
        console.print(f"‚ùå Erro ao iniciar agente: {e}", style="bold red")
        console.print("\nüí° Execute 'ollama serve' em outro terminal e tente novamente.")

if __name__ == "__main__":
    main()
